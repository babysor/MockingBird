## å®æ—¶è¯­éŸ³å…‹éš† - ä¸­æ–‡/æ™®é€šè¯
![mockingbird](https://user-images.githubusercontent.com/12797292/131216767-6eb251d6-14fc-4951-8324-2722f0cd4c63.jpg)

[![MIT License](https://img.shields.io/badge/license-MIT-blue.svg?style=flat)](http://choosealicense.com/licenses/mit/)

### [English](README.md)  | ä¸­æ–‡

### [DEMO VIDEO](https://www.bilibili.com/video/BV17Q4y1B7mY/) | [Wikiæ•™ç¨‹](https://github.com/babysor/MockingBird/wiki/Quick-Start-(Newbie)) ï½œ [è®­ç»ƒæ•™ç¨‹](https://vaj2fgg8yn.feishu.cn/docs/doccn7kAbr3SJz0KM0SIDJ0Xnhd)

## ç‰¹æ€§
ğŸŒ **ä¸­æ–‡** æ”¯æŒæ™®é€šè¯å¹¶ä½¿ç”¨å¤šç§ä¸­æ–‡æ•°æ®é›†è¿›è¡Œæµ‹è¯•ï¼šaidatatang_200zh, magicdata, aishell3, biaobei, MozillaCommonVoice, data_aishell ç­‰

ğŸ¤© **PyTorch** é€‚ç”¨äº pytorchï¼Œå·²åœ¨ 1.9.0 ç‰ˆæœ¬ï¼ˆæœ€æ–°äº 2021 å¹´ 8 æœˆï¼‰ä¸­æµ‹è¯•ï¼ŒGPU Tesla T4 å’Œ GTX 2060

ğŸŒ **Windows + Linux** å¯åœ¨ Windows æ“ä½œç³»ç»Ÿå’Œ linux æ“ä½œç³»ç»Ÿä¸­è¿è¡Œï¼ˆè‹¹æœç³»ç»ŸM1ç‰ˆä¹Ÿæœ‰ç¤¾åŒºæˆåŠŸè¿è¡Œæ¡ˆä¾‹ï¼‰

ğŸ¤© **Easy & Awesome** ä»…éœ€ä¸‹è½½æˆ–æ–°è®­ç»ƒåˆæˆå™¨ï¼ˆsynthesizerï¼‰å°±æœ‰è‰¯å¥½æ•ˆæœï¼Œå¤ç”¨é¢„è®­ç»ƒçš„ç¼–ç å™¨/å£°ç å™¨ï¼Œæˆ–å®æ—¶çš„HiFi-GANä½œä¸ºvocoder

ğŸŒ **Webserver Ready** å¯ä¼ºæœä½ çš„è®­ç»ƒç»“æœï¼Œä¾›è¿œç¨‹è°ƒç”¨

## å¼€å§‹
### 1. å®‰è£…è¦æ±‚
> æŒ‰ç…§åŸå§‹å­˜å‚¨åº“æµ‹è¯•æ‚¨æ˜¯å¦å·²å‡†å¤‡å¥½æ‰€æœ‰ç¯å¢ƒã€‚
è¿è¡Œå·¥å…·ç®±(demo_toolbox.py)éœ€è¦ **Python 3.7 æˆ–æ›´é«˜ç‰ˆæœ¬** ã€‚

* å®‰è£… [PyTorch](https://pytorch.org/get-started/locally/)ã€‚
> å¦‚æœåœ¨ç”¨ pip æ–¹å¼å®‰è£…çš„æ—¶å€™å‡ºç° `ERROR: Could not find a version that satisfies the requirement torch==1.9.0+cu102 (from versions: 0.1.2, 0.1.2.post1, 0.1.2.post2)` è¿™ä¸ªé”™è¯¯å¯èƒ½æ˜¯ python ç‰ˆæœ¬è¿‡ä½ï¼Œ3.9 å¯ä»¥å®‰è£…æˆåŠŸ
* å®‰è£… [ffmpeg](https://ffmpeg.org/download.html#get-packages)ã€‚
* è¿è¡Œ`pip install -r requirements.txt` æ¥å®‰è£…å‰©ä½™çš„å¿…è¦åŒ…ã€‚
* å®‰è£… webrtcvad `pip install webrtcvad-wheels`ã€‚

### 2. å‡†å¤‡é¢„è®­ç»ƒæ¨¡å‹
è€ƒè™‘è®­ç»ƒæ‚¨è‡ªå·±ä¸“å±çš„æ¨¡å‹æˆ–è€…ä¸‹è½½ç¤¾åŒºä»–äººè®­ç»ƒå¥½çš„æ¨¡å‹:
> è¿‘æœŸåˆ›å»ºäº†[çŸ¥ä¹ä¸“é¢˜](https://www.zhihu.com/column/c_1425605280340504576) å°†ä¸å®šæœŸæ›´æ–°ç‚¼ä¸¹å°æŠ€å·§orå¿ƒå¾—ï¼Œä¹Ÿæ¬¢è¿æé—®
#### 2.1 ä½¿ç”¨æ•°æ®é›†è‡ªå·±è®­ç»ƒencoderæ¨¡å‹ (å¯é€‰)

* è¿›è¡ŒéŸ³é¢‘å’Œæ¢…å°”é¢‘è°±å›¾é¢„å¤„ç†ï¼š
`python encoder_preprocess.py <datasets_root>`
ä½¿ç”¨`-d {dataset}` æŒ‡å®šæ•°æ®é›†ï¼Œæ”¯æŒ librispeech_otherï¼Œvoxceleb1ï¼Œaidatatang_200zhï¼Œä½¿ç”¨é€—å·åˆ†å‰²å¤„ç†å¤šæ•°æ®é›†ã€‚
* è®­ç»ƒencoder: `python encoder_train.py my_run <datasets_root>/SV2TTS/encoder`
> è®­ç»ƒencoderä½¿ç”¨äº†visdomã€‚ä½ å¯ä»¥åŠ ä¸Š`-no_visdom`ç¦ç”¨visdomï¼Œä½†æ˜¯æœ‰å¯è§†åŒ–ä¼šæ›´å¥½ã€‚åœ¨å•ç‹¬çš„å‘½ä»¤è¡Œ/è¿›ç¨‹ä¸­è¿è¡Œ"visdom"æ¥å¯åŠ¨visdomæœåŠ¡å™¨ã€‚

#### 2.2 ä½¿ç”¨æ•°æ®é›†è‡ªå·±è®­ç»ƒåˆæˆå™¨æ¨¡å‹ï¼ˆä¸2.3äºŒé€‰ä¸€ï¼‰
* ä¸‹è½½ æ•°æ®é›†å¹¶è§£å‹ï¼šç¡®ä¿æ‚¨å¯ä»¥è®¿é—® *train* æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰éŸ³é¢‘æ–‡ä»¶ï¼ˆå¦‚.wavï¼‰
* è¿›è¡ŒéŸ³é¢‘å’Œæ¢…å°”é¢‘è°±å›¾é¢„å¤„ç†ï¼š
`python pre.py <datasets_root> -d {dataset} -n {number}`
å¯ä¼ å…¥å‚æ•°ï¼š
* `-d {dataset}` æŒ‡å®šæ•°æ®é›†ï¼Œæ”¯æŒ aidatatang_200zh, magicdata, aishell3, data_aishell, ä¸ä¼ é»˜è®¤ä¸ºaidatatang_200zh
* `-n {number}` æŒ‡å®šå¹¶è¡Œæ•°ï¼ŒCPU 11770k + 32GBå®æµ‹10æ²¡æœ‰é—®é¢˜
> å‡å¦‚ä½ ä¸‹è½½çš„ `aidatatang_200zh`æ–‡ä»¶æ”¾åœ¨Dç›˜ï¼Œ`train`æ–‡ä»¶è·¯å¾„ä¸º `D:\data\aidatatang_200zh\corpus\train` , ä½ çš„`datasets_root`å°±æ˜¯ `D:\data\`

* è®­ç»ƒåˆæˆå™¨ï¼š
`python synthesizer_train.py mandarin <datasets_root>/SV2TTS/synthesizer`

* å½“æ‚¨åœ¨è®­ç»ƒæ–‡ä»¶å¤¹ *synthesizer/saved_models/* ä¸­çœ‹åˆ°æ³¨æ„çº¿æ˜¾ç¤ºå’ŒæŸå¤±æ»¡è¶³æ‚¨çš„éœ€è¦æ—¶ï¼Œè¯·è½¬åˆ°`å¯åŠ¨ç¨‹åº`ä¸€æ­¥ã€‚

#### 2.3ä½¿ç”¨ç¤¾åŒºé¢„å…ˆè®­ç»ƒå¥½çš„åˆæˆå™¨ï¼ˆä¸2.2äºŒé€‰ä¸€ï¼‰
> å½“å®åœ¨æ²¡æœ‰è®¾å¤‡æˆ–è€…ä¸æƒ³æ…¢æ…¢è°ƒè¯•ï¼Œå¯ä»¥ä½¿ç”¨ç¤¾åŒºè´¡çŒ®çš„æ¨¡å‹(æ¬¢è¿æŒç»­åˆ†äº«):

| ä½œè€… | ä¸‹è½½é“¾æ¥ | æ•ˆæœé¢„è§ˆ | ä¿¡æ¯ |
| --- | ----------- | ----- | ----- |
| ä½œè€… | https://pan.baidu.com/s/1iONvRxmkI-t1nHqxKytY3g  [ç™¾åº¦ç›˜é“¾æ¥](https://pan.baidu.com/s/1iONvRxmkI-t1nHqxKytY3g) 4j5d |  | 75k steps ç”¨3ä¸ªå¼€æºæ•°æ®é›†æ··åˆè®­ç»ƒ
| ä½œè€… | https://pan.baidu.com/s/1fMh9IlgKJlL2PIiRTYDUvw  [ç™¾åº¦ç›˜é“¾æ¥](https://pan.baidu.com/s/1fMh9IlgKJlL2PIiRTYDUvw) æå–ç ï¼šom7f |  | 25k steps ç”¨3ä¸ªå¼€æºæ•°æ®é›†æ··åˆè®­ç»ƒ, åˆ‡æ¢åˆ°tag v0.0.1ä½¿ç”¨
|@FawenYo | https://drive.google.com/file/d/1H-YGOUHpmqKxJ9FRc6vAjPuqQki24UbC/view?usp=sharing [ç™¾åº¦ç›˜é“¾æ¥](https://pan.baidu.com/s/1vSYXO4wsLyjnF3Unl-Xoxg) æå–ç ï¼š1024  | [input](https://github.com/babysor/MockingBird/wiki/audio/self_test.mp3) [output](https://github.com/babysor/MockingBird/wiki/audio/export.wav) | 200k steps å°æ¹¾å£éŸ³éœ€åˆ‡æ¢åˆ°tag v0.0.1ä½¿ç”¨
|@miven| https://pan.baidu.com/s/1PI-hM3sn5wbeChRryX-RCQ æå–ç ï¼š2021 | https://www.bilibili.com/video/BV1uh411B7AD/ | 150k steps æ³¨æ„ï¼šæ ¹æ®[issue](https://github.com/babysor/MockingBird/issues/37)ä¿®å¤ å¹¶åˆ‡æ¢åˆ°tag v0.0.1ä½¿ç”¨

#### 2.4è®­ç»ƒå£°ç å™¨ (å¯é€‰)
å¯¹æ•ˆæœå½±å“ä¸å¤§ï¼Œå·²ç»é¢„ç½®3æ¬¾ï¼Œå¦‚æœå¸Œæœ›è‡ªå·±è®­ç»ƒå¯ä»¥å‚è€ƒä»¥ä¸‹å‘½ä»¤ã€‚
* é¢„å¤„ç†æ•°æ®:
`python vocoder_preprocess.py <datasets_root> -m <synthesizer_model_path>`
> `<datasets_root>`æ›¿æ¢ä¸ºä½ çš„æ•°æ®é›†ç›®å½•ï¼Œ`<synthesizer_model_path>`æ›¿æ¢ä¸ºä¸€ä¸ªä½ æœ€å¥½çš„synthesizeræ¨¡å‹ç›®å½•ï¼Œä¾‹å¦‚ *sythensizer\saved_mode\xxx*


* è®­ç»ƒwavernnå£°ç å™¨:
`python vocoder_train.py <trainid> <datasets_root>`
> `<trainid>`æ›¿æ¢ä¸ºä½ æƒ³è¦çš„æ ‡è¯†ï¼ŒåŒä¸€æ ‡è¯†å†æ¬¡è®­ç»ƒæ—¶ä¼šå»¶ç»­åŸæ¨¡å‹

* è®­ç»ƒhifiganå£°ç å™¨:
`python vocoder_train.py <trainid> <datasets_root> hifigan`
> `<trainid>`æ›¿æ¢ä¸ºä½ æƒ³è¦çš„æ ‡è¯†ï¼ŒåŒä¸€æ ‡è¯†å†æ¬¡è®­ç»ƒæ—¶ä¼šå»¶ç»­åŸæ¨¡å‹

* è®­ç»ƒFre-GANå£°ç å™¨:
`python vocoder_train.py <trainid> <datasets_root> --config config.json fregan`
> `<trainid>`æ›¿æ¢ä¸ºä½ æƒ³è¦çš„æ ‡è¯†ï¼ŒåŒä¸€æ ‡è¯†å†æ¬¡è®­ç»ƒæ—¶ä¼šå»¶ç»­åŸæ¨¡å‹

### 3. å¯åŠ¨ç¨‹åºæˆ–å·¥å…·ç®±
æ‚¨å¯ä»¥å°è¯•ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤ï¼š

### 3.1 å¯åŠ¨Webç¨‹åºï¼š
`python web.py`
è¿è¡ŒæˆåŠŸååœ¨æµè§ˆå™¨æ‰“å¼€åœ°å€, é»˜è®¤ä¸º `http://localhost:8080`
![123](https://user-images.githubusercontent.com/12797292/135494044-ae59181c-fe3a-406f-9c7d-d21d12fdb4cb.png)
> æ³¨ï¼šç›®å‰ç•Œé¢æ¯”è¾ƒbuggy, 
> * ç¬¬ä¸€æ¬¡ç‚¹å‡»`å½•åˆ¶`è¦ç­‰å¾…å‡ ç§’æµè§ˆå™¨æ­£å¸¸å¯åŠ¨å½•éŸ³ï¼Œå¦åˆ™ä¼šæœ‰é‡éŸ³
> * å½•åˆ¶ç»“æŸä¸è¦å†ç‚¹`å½•åˆ¶`è€Œæ˜¯`åœæ­¢`
> * ä»…æ”¯æŒæ‰‹åŠ¨æ–°å½•éŸ³ï¼ˆ16khzï¼‰, ä¸æ”¯æŒè¶…è¿‡4MBçš„å½•éŸ³ï¼Œæœ€ä½³é•¿åº¦åœ¨5~15ç§’
> * é»˜è®¤ä½¿ç”¨ç¬¬ä¸€ä¸ªæ‰¾åˆ°çš„æ¨¡å‹ï¼Œæœ‰åŠ¨æ‰‹èƒ½åŠ›çš„å¯ä»¥çœ‹ä»£ç ä¿®æ”¹ `web\__init__.py`ã€‚

### 3.2 å¯åŠ¨å·¥å…·ç®±ï¼š
`python demo_toolbox.py -d <datasets_root>`
> è¯·æŒ‡å®šä¸€ä¸ªå¯ç”¨çš„æ•°æ®é›†æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœæœ‰æ”¯æŒçš„æ•°æ®é›†åˆ™ä¼šè‡ªåŠ¨åŠ è½½ä¾›è°ƒè¯•ï¼Œä¹ŸåŒæ—¶ä¼šä½œä¸ºæ‰‹åŠ¨å½•åˆ¶éŸ³é¢‘çš„å­˜å‚¨ç›®å½•ã€‚

<img width="1042" alt="d48ea37adf3660e657cfb047c10edbc" src="https://user-images.githubusercontent.com/7423248/134275227-c1ddf154-f118-4b77-8949-8c4c7daf25f0.png">

### 4. ç•ªå¤–ï¼šè¯­éŸ³è½¬æ¢Voice Conversion(PPG based)
æƒ³åƒæŸ¯å—æ‹¿ç€å˜å£°å™¨ç„¶åå‘å‡ºæ¯›åˆ©å°äº”éƒçš„å£°éŸ³å—ï¼Ÿæœ¬é¡¹ç›®ç°åŸºäºPPG-VCï¼Œå¼•å…¥é¢å¤–ä¸¤ä¸ªæ¨¡å—ï¼ˆPPG extractor + PPG2Melï¼‰, å¯ä»¥å®ç°å˜å£°åŠŸèƒ½ã€‚ï¼ˆæ–‡æ¡£ä¸å…¨ï¼Œå°¤å…¶æ˜¯è®­ç»ƒéƒ¨åˆ†ï¼Œæ­£åœ¨åŠªåŠ›è¡¥å……ä¸­ï¼‰
#### 4.0 å‡†å¤‡ç¯å¢ƒ
* ç¡®ä¿é¡¹ç›®ä»¥ä¸Šç¯å¢ƒå·²ç»å®‰è£…okï¼Œè¿è¡Œ`pip install -r requirements_vc.txt` æ¥å®‰è£…å‰©ä½™çš„å¿…è¦åŒ…ã€‚
* ä¸‹è½½ä»¥ä¸‹æ¨¡å‹ é“¾æ¥ï¼šhttps://pan.baidu.com/s/1bl_x_DHJSAUyN2fma-Q_Wg 
æå–ç ï¼šgh41
  * 24Ké‡‡æ ·ç‡ä¸“ç”¨çš„vocoderï¼ˆhifiganï¼‰åˆ° *vocoder\saved_mode\xxx*
  * é¢„è®­ç»ƒçš„ppgç‰¹å¾encoder(ppg_extractor)åˆ° *ppg_extractor\saved_mode\xxx*
  * é¢„è®­ç»ƒçš„PPG2Melåˆ° *ppg2mel\saved_mode\xxx*

#### 4.1 ä½¿ç”¨æ•°æ®é›†è‡ªå·±è®­ç»ƒPPG2Melæ¨¡å‹ (å¯é€‰)

* ä¸‹è½½aidatatang_200zhæ•°æ®é›†å¹¶è§£å‹ï¼šç¡®ä¿æ‚¨å¯ä»¥è®¿é—® *train* æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰éŸ³é¢‘æ–‡ä»¶ï¼ˆå¦‚.wavï¼‰
* è¿›è¡ŒéŸ³é¢‘å’Œæ¢…å°”é¢‘è°±å›¾é¢„å¤„ç†ï¼š
`python pre4ppg.py <datasets_root> -d {dataset} -n {number}`
å¯ä¼ å…¥å‚æ•°ï¼š
* `-d {dataset}` æŒ‡å®šæ•°æ®é›†ï¼Œæ”¯æŒ aidatatang_200zh, ä¸ä¼ é»˜è®¤ä¸ºaidatatang_200zh
* `-n {number}` æŒ‡å®šå¹¶è¡Œæ•°ï¼ŒCPU 11770kåœ¨8çš„æƒ…å†µä¸‹ï¼Œéœ€è¦è¿è¡Œ12åˆ°18å°æ—¶ï¼å¾…ä¼˜åŒ–
> å‡å¦‚ä½ ä¸‹è½½çš„ `aidatatang_200zh`æ–‡ä»¶æ”¾åœ¨Dç›˜ï¼Œ`train`æ–‡ä»¶è·¯å¾„ä¸º `D:\data\aidatatang_200zh\corpus\train` , ä½ çš„`datasets_root`å°±æ˜¯ `D:\data\`

* è®­ç»ƒåˆæˆå™¨, æ³¨æ„åœ¨ä¸Šä¸€æ­¥å…ˆä¸‹è½½å¥½`ppg2mel.yaml`, ä¿®æ”¹é‡Œé¢çš„åœ°å€æŒ‡å‘é¢„è®­ç»ƒå¥½çš„æ–‡ä»¶å¤¹ï¼š
`python ppg2mel_train.py --config .\ppg2mel\saved_models\ppg2mel.yaml --oneshotvc `
* å¦‚æœæƒ³è¦ç»§ç»­ä¸Šä¸€æ¬¡çš„è®­ç»ƒï¼Œå¯ä»¥é€šè¿‡`--load .\ppg2mel\saved_models\<old_pt_file>` å‚æ•°æŒ‡å®šä¸€ä¸ªé¢„è®­ç»ƒæ¨¡å‹æ–‡ä»¶ã€‚

#### 4.2 å¯åŠ¨å·¥å…·ç®±VCæ¨¡å¼
æ‚¨å¯ä»¥å°è¯•ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤ï¼š
`python demo_toolbox.py vc -d <datasets_root>`
> è¯·æŒ‡å®šä¸€ä¸ªå¯ç”¨çš„æ•°æ®é›†æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœæœ‰æ”¯æŒçš„æ•°æ®é›†åˆ™ä¼šè‡ªåŠ¨åŠ è½½ä¾›è°ƒè¯•ï¼Œä¹ŸåŒæ—¶ä¼šä½œä¸ºæ‰‹åŠ¨å½•åˆ¶éŸ³é¢‘çš„å­˜å‚¨ç›®å½•ã€‚
<img width="971" alt="å¾®ä¿¡å›¾ç‰‡_20220305005351" src="https://user-images.githubusercontent.com/7423248/156805733-2b093dbc-d989-4e68-8609-db11f365886a.png">

## å¼•ç”¨åŠè®ºæ–‡
> è¯¥åº“ä¸€å¼€å§‹ä»ä»…æ”¯æŒè‹±è¯­çš„[Real-Time-Voice-Cloning](https://github.com/CorentinJ/Real-Time-Voice-Cloning) åˆ†å‰å‡ºæ¥çš„ï¼Œé¸£è°¢ä½œè€…ã€‚

| URL | Designation | æ ‡é¢˜ | å®ç°æºç  |
| --- | ----------- | ----- | --------------------- |
| [1803.09017](https://arxiv.org/abs/1803.09017) | GlobalStyleToken (synthesizer)| Style Tokens: Unsupervised Style Modeling, Control and Transfer in End-to-End Speech Synthesis | æœ¬ä»£ç åº“ |
| [2010.05646](https://arxiv.org/abs/2010.05646) | HiFi-GAN (vocoder)| Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis | æœ¬ä»£ç åº“ |
|[**1806.04558**](https://arxiv.org/pdf/1806.04558.pdf) | SV2TTS | Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis | æœ¬ä»£ç åº“ |
|[1802.08435](https://arxiv.org/pdf/1802.08435.pdf) | WaveRNN (vocoder) | Efficient Neural Audio Synthesis | [fatchord/WaveRNN](https://github.com/fatchord/WaveRNN) |
|[1703.10135](https://arxiv.org/pdf/1703.10135.pdf) | Tacotron (synthesizer) | Tacotron: Towards End-to-End Speech Synthesis | [fatchord/WaveRNN](https://github.com/fatchord/WaveRNN)
|[1710.10467](https://arxiv.org/pdf/1710.10467.pdf) | GE2E (encoder)| Generalized End-To-End Loss for Speaker Verification | æœ¬ä»£ç åº“ |

## å¸¸è¦‹å•é¡Œ(FQ&A)
#### 1.æ•¸æ“šé›†å“ªè£¡ä¸‹è¼‰?
| æ•°æ®é›† | OpenSLRåœ°å€ | å…¶ä»–æº (Google Drive, Baiduç½‘ç›˜ç­‰) |
| --- | ----------- | ---------------|
| aidatatang_200zh | [OpenSLR](http://www.openslr.org/62/) | [Google Drive](https://drive.google.com/file/d/110A11KZoVe7vy6kXlLb6zVPLb_J91I_t/view?usp=sharing) |
| magicdata | [OpenSLR](http://www.openslr.org/68/) | [Google Drive (Dev set)](https://drive.google.com/file/d/1g5bWRUSNH68ycC6eNvtwh07nX3QhOOlo/view?usp=sharing) |
| aishell3 | [OpenSLR](https://www.openslr.org/93/) | [Google Drive](https://drive.google.com/file/d/1shYp_o4Z0X0cZSKQDtFirct2luFUwKzZ/view?usp=sharing) |
| data_aishell | [OpenSLR](https://www.openslr.org/33/) |  |
> è§£å£“ aidatatang_200zh å¾Œï¼Œé‚„éœ€å°‡ `aidatatang_200zh\corpus\train`ä¸‹çš„æª”æ¡ˆå…¨é¸è§£å£“ç¸®

#### 2.`<datasets_root>`æ˜¯ä»€éº¼æ„æ€?
å‡å¦‚æ•¸æ“šé›†è·¯å¾‘ç‚º `D:\data\aidatatang_200zh`ï¼Œé‚£éº¼ `<datasets_root>`å°±æ˜¯ `D:\data`

#### 3.è¨“ç·´æ¨¡å‹é¡¯å­˜ä¸è¶³
è¨“ç·´åˆæˆå™¨æ™‚ï¼šå°‡ `synthesizer/hparams.py`ä¸­çš„batch_sizeåƒæ•¸èª¿å°
```
//èª¿æ•´å‰
tts_schedule = [(2,  1e-3,  20_000,  12),   # Progressive training schedule
                (2,  5e-4,  40_000,  12),   # (r, lr, step, batch_size)
                (2,  2e-4,  80_000,  12),   #
                (2,  1e-4, 160_000,  12),   # r = reduction factor (# of mel frames
                (2,  3e-5, 320_000,  12),   #     synthesized for each decoder iteration)
                (2,  1e-5, 640_000,  12)],  # lr = learning rate
//èª¿æ•´å¾Œ
tts_schedule = [(2,  1e-3,  20_000,  8),   # Progressive training schedule
                (2,  5e-4,  40_000,  8),   # (r, lr, step, batch_size)
                (2,  2e-4,  80_000,  8),   #
                (2,  1e-4, 160_000,  8),   # r = reduction factor (# of mel frames
                (2,  3e-5, 320_000,  8),   #     synthesized for each decoder iteration)
                (2,  1e-5, 640_000,  8)],  # lr = learning rate
```

è²ç¢¼å™¨-é è™•ç†æ•¸æ“šé›†æ™‚ï¼šå°‡ `synthesizer/hparams.py`ä¸­çš„batch_sizeåƒæ•¸èª¿å°
```
//èª¿æ•´å‰
### Data Preprocessing
        max_mel_frames = 900,
        rescale = True,
        rescaling_max = 0.9,
        synthesis_batch_size = 16,                  # For vocoder preprocessing and inference.
//èª¿æ•´å¾Œ
### Data Preprocessing
        max_mel_frames = 900,
        rescale = True,
        rescaling_max = 0.9,
        synthesis_batch_size = 8,                  # For vocoder preprocessing and inference.
```

è²ç¢¼å™¨-è¨“ç·´è²ç¢¼å™¨æ™‚ï¼šå°‡ `vocoder/wavernn/hparams.py`ä¸­çš„batch_sizeåƒæ•¸èª¿å°
```
//èª¿æ•´å‰
# Training
voc_batch_size = 100
voc_lr = 1e-4
voc_gen_at_checkpoint = 5
voc_pad = 2

//èª¿æ•´å¾Œ
# Training
voc_batch_size = 6
voc_lr = 1e-4
voc_gen_at_checkpoint = 5
voc_pad =2
```

#### 4.ç¢°åˆ°`RuntimeError: Error(s) in loading state_dict for Tacotron: size mismatch for encoder.embedding.weight: copying a param with shape torch.Size([70, 512]) from checkpoint, the shape in current model is torch.Size([75, 512]).`
è«‹åƒç…§ issue [#37](https://github.com/babysor/MockingBird/issues/37)

#### 5.å¦‚ä½•æ”¹å–„CPUã€GPUä½”ç”¨ç‡?
é©æƒ…æ³èª¿æ•´batch_sizeåƒæ•¸ä¾†æ”¹å–„

#### 6.ç™¼ç”Ÿ `é é¢æ–‡ä»¶å¤ªå°ï¼Œç„¡æ³•å®Œæˆæ“ä½œ`
è«‹åƒè€ƒé€™ç¯‡[æ–‡ç« ](https://blog.csdn.net/qq_17755303/article/details/112564030)ï¼Œå°‡è™›æ“¬å…§å­˜æ›´æ”¹ç‚º100G(102400)ï¼Œä¾‹å¦‚:æ¡£æ¡ˆæ”¾ç½®Dæ§½å°±æ›´æ”¹Dæ§½çš„è™šæ‹Ÿå†…å­˜

#### 7.ä»€ä¹ˆæ—¶å€™ç®—è®­ç»ƒå®Œæˆï¼Ÿ
é¦–å…ˆä¸€å®šè¦å‡ºç°æ³¨æ„åŠ›æ¨¡å‹ï¼Œå…¶æ¬¡æ˜¯lossè¶³å¤Ÿä½ï¼Œå–å†³äºç¡¬ä»¶è®¾å¤‡å’Œæ•°æ®é›†ã€‚æ‹¿æœ¬äººçš„ä¾›å‚è€ƒï¼Œæˆ‘çš„æ³¨æ„åŠ›æ˜¯åœ¨ 18k æ­¥ä¹‹åå‡ºç°çš„ï¼Œå¹¶ä¸”åœ¨ 50k æ­¥ä¹‹åæŸå¤±å˜å¾—ä½äº 0.4
![attention_step_20500_sample_1](https://user-images.githubusercontent.com/7423248/128587252-f669f05a-f411-4811-8784-222156ea5e9d.png)

![step-135500-mel-spectrogram_sample_1](https://user-images.githubusercontent.com/7423248/128587255-4945faa0-5517-46ea-b173-928eff999330.png)

